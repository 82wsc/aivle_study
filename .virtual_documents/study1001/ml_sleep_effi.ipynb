import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# =========================================
from sklearn.impute import KNNImputer
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import *
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import *
import pickle
from sklearn.ensemble import StackingRegressor
from sklearn.ensemble import VotingRegressor





data = pd.read_csv('Sleep_Efficiency.csv')


data.head()


data.shape


data.info()


data.corr(numeric_only=True).style.background_gradient()








data.drop('ID',axis=1,inplace=True)


col = data.columns


# KNNImputer 사용위한 전처리


# Gender 
# female: 0 / male: 1
data['Gender'].replace({'Female':0,'Male':1},inplace = True)


data['Gender'].value_counts()


data['Bedtime'] = pd.to_datetime(data['Bedtime'])
data['Wakeup time'] = pd.to_datetime(data['Wakeup time'])


data['Bedtime'] = data['Bedtime'].dt.hour + data['Bedtime'].dt.minute/60
data['Wakeup time'] = data['Wakeup time'].dt.hour +data['Wakeup time'].dt.minute/60


data['Bedtime'].value_counts()


data['Wakeup time'].value_counts()


# Smoking Status
# No: 0, Yes: 1
data['Smoking status']=data['Smoking status'].replace({'No':0,'Yes':1})


data['Smoking status'].value_counts()





data.isna().sum()


imputer = KNNImputer(n_neighbors = 5)


data = imputer.fit_transform(data)


data= pd.DataFrame(data, columns=col)


data.isna().sum()





data.info()


data.head()


data['Sleep time'] = data['Wakeup time']-data['Bedtime']


data['Sleep time'].value_counts()


data['Sleep time'] = data['Sleep time'].apply(lambda x: x + 24 if x < 0 else x)


data['Sleep time'].value_counts()


data.drop(['Wakeup time','Bedtime'],axis=1,inplace=True)


data.columns





data.describe().T


col = data.columns


scaler = StandardScaler()
data_sc = scaler.fit_transform(data)
data_sc = pd.DataFrame(data_sc, columns=col)


data_sc.head()


# target 확인
target = 'Sleep efficiency'

# 데이터 분리
x = data.drop(columns=target)
y = data.loc[:, target]


# train,val,test로 5:2:3


x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.5, random_state=1)


x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.6, random_state=1)








model_k = KNeighborsRegressor()


model_k.fit(x_train, y_train)


y_pred = model_k.predict(x_val)


print("MAE:",mean_absolute_error(y_val, y_pred))
print("R2:",r2_score(y_val, y_pred))





y_t = model_k.predict(x_test)


print("MAE:",mean_absolute_error(y_test, y_t))
print("R2:",r2_score(y_test, y_t))





# 성능예측
cv_score = cross_val_score(KNeighborsRegressor(),x_train,y_train,cv=10)


print(cv_score)
print(cv_score.mean())


param = {'n_neighbors' : range(1,50),
         'weights': ['uniform', 'distance'], #'distance'로 설정하면 이웃 간의 거리의 역수에 따라 가중치
         'metric': ['minkowski','euclidean','manhattan','chebyshev'], 
         'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], #ball_tree와 kd_tree는 데이터 구조를 사용해 검색을 최적화하며, brute는 단순히 모든 포인트와 거리를 계산
         'leaf_size': range(5,30,5) #ball_tree와 kd_tree 알고리즘에서 사용하는 리프의 크기
        }

model_knn = GridSearchCV(KNeighborsRegressor(),
                     param,
                     cv=5,
                     scoring=['r2', 'neg_mean_absolute_error'],  # MAE는 부호를 바꿔서 사용
                     refit='r2',
                     n_jobs=-1)  # 기본적으로 MAE 기준으로 최적 모델 선택

model_knn.fit(x_train, y_train)


# 예측 결과 확인
print(model_knn.best_params_)
print(model_knn.best_score_)





result={}


y_t = model_knn.best_estimator_.predict(x_test)


result['knn']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result





with open('model_knn.pkl', 'wb') as file:
    pickle.dump(model_knn.best_estimator_, file)


# 모델 불러오기
with open('model_knn.pkl', 'rb') as file:
    loaded_model_knn = pickle.load(file)








model_s = SVR()


model_s.fit(x_train, y_train)


y_pred = model_s.predict(x_val)


print("MAE:",mean_absolute_error(y_val, y_pred))
print("R2:",r2_score(y_val, y_pred))





y_t = model_k.predict(x_test)


print("MAE:",mean_absolute_error(y_test, y_t))
print("R2:",r2_score(y_test, y_t))





# 성능예측
cv_score = cross_val_score(SVR(),x_train,y_train,cv=10)


print(cv_score)
print(cv_score.mean())


param_grid = {'C': [0.01, 1, 100],
              'kernel': ['linear', 'rbf','poly','sigmoid'],
              'degree': [2, 3, 4], #다항 커널 (poly)을 사용할 때 다항식의 차수를 지정
              'gamma': ['scale', 'auto', 0.01, 0.1, 1], #rbf, poly, sigmoid 커널에서 사용되는 매개변수로, 데이터 포인트의 영향 범위
              'epsilon': [0.1, 0.2, 0.5], #오차 허용 범위
              'shrinking': [True,False], #휴리스틱을 사용해 최적화를 수행할지 여부를
              'coef0': [0, 0.5, 1], #다항 커널 (poly) 및 시그모이드 커널 (sigmoid)에서 상수 항을 조정
             }


model_svr = GridSearchCV ( SVR(),
                          param_grid,
                          cv=10,
                          scoring=['r2', 'neg_mean_absolute_error'],  # MAE는 부호를 바꿔서 사용
                          refit='r2',
                          n_jobs=-1)

#학습
# train data로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .
model_svr.fit(x_train, y_train)


# 예측 결과 확인
print(model_svr.best_params_)
print(model_svr.best_score_)





y_t = model_svr.best_estimator_.predict(x_test)


result['svr']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result





with open('model_svr.pkl', 'wb') as file:
    pickle.dump(model_svr.best_estimator_, file)


# 모델 불러오기
with open('model_svr.pkl', 'rb') as file:
    loaded_model_svr = pickle.load(file)








stacking_regressor = StackingRegressor(
    estimators=[('knn',loaded_model_knn), ('svr',loaded_model_svr)],
    final_estimator=LinearRegression()
)


# 모델 학습
stacking_regressor.fit(x_train, y_train)


# 예측 및 성능 평가
stacking_pred = stacking_regressor.predict(x_val)
print("Stacking Regressor MSE:", mean_squared_error(y_val, stacking_pred))
print("R2:",r2_score(y_val, stacking_pred))





y_t = stacking_regressor.predict(x_test)


result['es:k+s']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result





stacking_regressor2 = StackingRegressor(
    estimators=[('knn_1',loaded_model_knn), ('knn_2',loaded_model_knn)],
    final_estimator=LinearRegression()
)


# 모델 학습
stacking_regressor2.fit(x_train, y_train)


# 예측 및 성능 평가
stacking_pred2 = stacking_regressor2.predict(x_val)
print("Stacking Regressor MSE:", mean_squared_error(y_val, stacking_pred2))
print("R2:",r2_score(y_val, stacking_pred2))





y_t = stacking_regressor2.predict(x_test)


result['es:k+k']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result





stacking_regressor3 = StackingRegressor(
    estimators=[('svr_1',loaded_model_svr), ('svr_2',loaded_model_svr)],
    final_estimator=LinearRegression()
)


# 모델 학습
stacking_regressor3.fit(x_train, y_train)


# 예측 및 성능 평가
stacking_pred3 = stacking_regressor3.predict(x_val)
print("Stacking Regressor MSE:", mean_squared_error(y_val, stacking_pred3))
print("R2:",r2_score(y_val, stacking_pred3))





y_t = stacking_regressor3.predict(x_test)


result['es:s+s']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result


# with open('model_ens.pkl', 'wb') as file:
#     pickle.dump(stacking_regressor, file)








# 보팅 모델 선언
estimators = [('knn',loaded_model_knn), ('svr_2',loaded_model_svr)]

voting_regressor1 = VotingRegressor(estimators=estimators)

# 학습하기
voting_regressor1.fit(x_train, y_train)



# 예측 및 성능 평가
voting_pred1 = voting_regressor1.predict(x_val)
print("Stacking Regressor MSE:", mean_squared_error(y_val, voting_pred1))
print("R2:",r2_score(y_val, voting_pred1))





y_t = voting_regressor1.predict(x_test)


result['ev:k+s']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result





# 보팅 모델 선언
estimators = [('svr_1',loaded_model_svr), ('svr_2',loaded_model_svr)]

voting_regressor2 = VotingRegressor(estimators=estimators)

# 학습하기
voting_regressor2.fit(x_train, y_train)



# 예측 및 성능 평가
voting_pred2 = voting_regressor2.predict(x_val)
print("Stacking Regressor MSE:", mean_squared_error(y_val, voting_pred2))
print("R2:",r2_score(y_val, voting_pred2))





y_t = voting_regressor2.predict(x_test)


result['ev:s+s']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result





# 보팅 모델 선언
estimators = [('knn_1',loaded_model_knn), ('knn_2',loaded_model_knn)]

voting_regressor3 = VotingRegressor(estimators=estimators)

# 학습하기
voting_regressor3.fit(x_train, y_train)



# 예측 및 성능 평가
voting_pred3 = voting_regressor3.predict(x_val)
print("Stacking Regressor MSE:", mean_squared_error(y_val, voting_pred3))
print("R2:",r2_score(y_val, voting_pred3))





y_t = voting_regressor3.predict(x_test)


result['ev:k+k']=(mean_absolute_error(y_test, y_t),r2_score(y_test, y_t))


result


sorted_data = sorted(
    {key: round(value[1], 3) for key, value in result.items()}.items(),
    key=lambda x: x[1],
    reverse=True
)

# 레이블과 값 분리
labels, values = zip(*sorted_data)

# 그래프 그리기
plt.figure(figsize=(10, 6))
plt.bar(labels, values, color='skyblue')
plt.xlabel('Methods')
plt.ylabel('Values')
plt.title('R2')
plt.ylim(0.73, 0.77)  # y축 범위 설정
plt.show()


sorted_data = sorted(
    {key: round(value[0], 3) for key, value in result.items()}.items(),
    key=lambda x: x[1],
    reverse=True
)

# 레이블과 값 분리
labels, values = zip(*sorted_data)

# 그래프 그리기
plt.figure(figsize=(10, 6))
plt.bar(labels, values, color='skyblue')
plt.xlabel('Methods')
plt.ylabel('Values')
plt.title('MAE')
plt.show()









